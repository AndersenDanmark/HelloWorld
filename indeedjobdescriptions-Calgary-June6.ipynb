{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from goose3 import Goose\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import pprint\n",
    "from flask import Flask\n",
    "from flask import send_from_directory\n",
    "from flask import request\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat an object called jobs to dump scraped job descriptions to it\n",
    "jobs={}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prevent the driver stopping due to the unexpectedAlertBehaviour.# preven \n",
    "webdriver.DesiredCapabilities.FIREFOX[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "get_info = True\n",
    "\n",
    "DRIVER_EXE = r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\data_skills\\geckodriver.exe\"\n",
    "#the instance of Firefox WebDriver is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent the driver stopping due to the unexpectedAlertBehaviour.\n",
    "webdriver.DesiredCapabilities.FIREFOX[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "get_info = True\n",
    "\n",
    "DRIVER_EXE = r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\data_skills\\geckodriver.exe\"\n",
    "#the instance of Firefox WebDriver is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build connection\n",
    "#client = MongoClient('mongodb://127.0.0.1:27017')\n",
    "\n",
    "client =MongoClient(\"mongodb://andrewyan:andrewyan!23@ds237660.mlab.com:37660/heroku_zdtgskz7\")\n",
    "\n",
    "#creating a database called jobs_database\n",
    "db = client.heroku_zdtgskz7\n",
    "\n",
    "#creating a collection (table) called collection\n",
    "collection = db.datasciencejobs\n",
    "\n",
    "#job_id = db.collection.insert_one(jobs).inserted_id\n",
    "#collection.find_one({\"Job Title\": \"Data Scientist\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change arguments inside URLs\n",
    "\n",
    "base_url = \"https://ca.indeed.com/jobs\" \n",
    "\n",
    "original_url = \"http://www.indeed.com/\"\n",
    "\n",
    "job_search_keywords=\"software developer\"\n",
    "\n",
    "job_search_sortbydate=\"date\" #indeed's default sort method is relevance\n",
    "\n",
    "job_search_location='Calgary,AB'\n",
    "\n",
    "# fromage value below, is age of jobs published,values are 'Any' or '15' or '7' or '3' or '1'\n",
    "job_on_market_days='Any'\n",
    "\n",
    "#limit value below, is the number of jobs to be displayed per page, values are \"10\", \"20\", \"30\", \"50\"\n",
    "job_result_per_page=10\n",
    "\n",
    "search_result_pg=0 #search_result_pg=20 is the 3rd page, the 1st page value is 0, 2nd page value is 10.\n",
    "\n",
    "mydict={'q':job_search_keywords,\n",
    "       'l':job_search_location,\n",
    "        'fromage':job_on_market_days,\n",
    "        'limit':job_result_per_page,        \n",
    "        'sort':job_search_sortbydate,\n",
    "       'start':search_result_pg}\n",
    "\n",
    "resp_make_url=requests.get(base_url,params=mydict)\n",
    "\n",
    "searched_url=resp_make_url.url\n",
    "\n",
    "print(searched_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to change arguments inside URLs#to chan \n",
    "\n",
    "base_url = \"https://ca.indeed.com/jobs\" \n",
    "\n",
    "original_url = \"http://www.indeed.com/\"\n",
    "\n",
    "job_search_keywords='\"software developer\"'\n",
    "\n",
    "\n",
    "#job_search_keywords='\"fullstack\" or \"full stack\" or \"software\" or \"programmer\" or \"web developer\" or \"data scientist\" or \"software engineer\" or \"web application\" or \"software developer\" or \"java developer\"'\n",
    "\n",
    "job_search_sortbydate=\"date\" #indeed's default sort method is relevance\n",
    "\n",
    "job_search_location='Calgary,AB'\n",
    "\n",
    "# fromage value below, is age of jobs published,values are 'Any' or '15' or '7' or '3' or '1'\n",
    "job_on_market_days='Any'\n",
    "\n",
    "#limit value below, is the number of jobs to be displayed per page, values are \"10\", \"20\", \"30\", \"50\"\n",
    "job_result_per_page=10\n",
    "\n",
    "search_result_pg=0 #search_result_pg=20 is the 3rd page, the 1st page value is 0, 2nd page value is 10.\n",
    "\n",
    "mydict={'q':job_search_keywords,\n",
    "       'l':job_search_location,\n",
    "        'fromage':job_on_market_days,\n",
    "        'limit':job_result_per_page,        \n",
    "        'sort':job_search_sortbydate,\n",
    "       'start':search_result_pg}\n",
    "\n",
    "resp_make_url=requests.get(base_url,params=mydict)\n",
    "\n",
    "searched_url=resp_make_url.url\n",
    "\n",
    "print(searched_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Firefox\n",
    "driver=webdriver.Firefox(executable_path=DRIVER_EXE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "page_number=0\n",
    "\n",
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#change the rage to change number of search result pages to iterate\n",
    "\n",
    "for page_number in range(0,1):\n",
    "    \n",
    "    mydict={'q':job_search_keywords,\n",
    "           'l':job_search_location,\n",
    "            'fromage':job_on_market_days,\n",
    "            'limit':job_result_per_page,        \n",
    "            'sort':job_search_sortbydate,\n",
    "           'start':search_result_pg}\n",
    "\n",
    "    resp_make_url=requests.get(base_url,params=mydict)\n",
    "    \n",
    "    searched_url=resp_make_url.url\n",
    "    \n",
    "    \n",
    "#loop through one page of searched results:\n",
    "    \n",
    "    #creating a response object called resp\n",
    "    resp = requests.get(searched_url)\n",
    "\n",
    "    #You can find out what encoding Requests is using, and change it, using the r.encoding property\n",
    "    #print(resp.encoding)\n",
    "\n",
    "    # Running the url link through BeautifulSoup give us a BeautifulSoup object, which represents the document as a nested data structure.\n",
    "    start_soup = BeautifulSoup(resp.content)\n",
    "    urls = start_soup.findAll('a',{'rel':'nofollow','target':'_blank'}) #this are the links of the job posts\n",
    "\n",
    "    urls = [link['href'] for link in urls] \n",
    "    \n",
    "    #The line above is equivalent to below:\n",
    "    \n",
    "    #To store all url links to a list called links.\n",
    "        #urls=[]\n",
    "        #for link in urls:\n",
    "        #    urls.append(link.get('href'))\n",
    "        \n",
    "        #print(urls[0:2])\n",
    "  \n",
    "    \n",
    "    #print job links of the current search result page:\n",
    "    print(\"total # of jobs on this page is :\"+str(len(urls)))\n",
    "\n",
    "    \n",
    "\n",
    "       \n",
    "    for url in urls: #change it back to for i in range(len(urls)): \n",
    "        get_info = True\n",
    "        try:\n",
    "            driver.get(original_url+url) #The driver.get method will navigate to a page given by the URL.\n",
    "        except TimeoutException:\n",
    "            get_info = False\n",
    "        except IndexError:\n",
    "            get_info = False\n",
    "            continue\n",
    "        j = random.randint(1500,3200)/1000.0 #change these values to avoid anti-scraping\n",
    "\n",
    "\n",
    "        time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "\n",
    "\n",
    "        if get_info:\n",
    "            soup=BeautifulSoup(driver.page_source)\n",
    "            \n",
    "            job_id_values=soup.find(\"div\",{\"class\":\"edit_note_content\"})\n",
    "            job_id=job_id_values['id'][11:]\n",
    "            #job_id_values data type is tag, to access a tag's attribute values use tag['attribute_key']\n",
    "            \n",
    "\n",
    "            '''\n",
    "            print(\"job_id_values  is:\")\n",
    "            print(job_id_values)\n",
    "            print(job_id_values['id'])\n",
    "            print(\"job_id_values type is:\")\n",
    "            print(type(job_id_values))\n",
    "            print(\"job_id is:\")\n",
    "            print(job_id)\n",
    "            print(\"job_id type is:\")\n",
    "            print(type(job_id))\n",
    "            '''\n",
    "\n",
    "           \n",
    "            job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "            print(job_title.text)\n",
    "\n",
    "\n",
    "            company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "            print(company.text)\n",
    "\n",
    "            location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "            print(location.text)\n",
    "\n",
    "            job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "\n",
    "            #print(job_description.text)\n",
    "\n",
    "            #print (driver.current_url)\n",
    "\n",
    "            jobs={\n",
    "                \"Job ID\":job_id,\n",
    "                \"Job Title\":job_title.text,\n",
    "                \"Company Name\":company.text,\n",
    "                \"Location\":location.text,\n",
    "                \"Url\":driver.current_url,\n",
    "                \"Job Description\":job_description.text\n",
    "\n",
    "            }\n",
    "            #This line inserts each job into MongoDB\n",
    "            #pprint.pprint(jobs)\n",
    "            collection.update({\"Job ID\":job_id},jobs,upsert=True)\n",
    "         \n",
    "    \n",
    "#move to the next page of searched results, after iterating through all jobs links in current page\n",
    "    search_result_pg+=len(urls)\n",
    "    \n",
    "    print(page_number)\n",
    "    \n",
    "    print(searched_url) # print urls for search result page 1, 2 ,3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test a webpage\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "driver=webdriver.Firefox(executable_path=DRIVER_EXE)\n",
    "\n",
    "#Enter the url below\n",
    "\n",
    "test_url=\"https://www.indeed.com/rc/clk?jk=2188071b67e80bef&fccid=2b585dc0d549b736&vjs=3\"\n",
    "#test_url=\"https://www.indeed.com/viewjob?jk=fb2e3433eade4b6c&tk=1cf6na4o9btcr9g6&from=serp&alid=3&advn=8231918522092453\"\n",
    "    \n",
    "    \n",
    "#resp = requests.get(test_url)\n",
    "\n",
    "    #You can find out what encoding Requests is using, and change it, using the r.encoding property\n",
    "    #print(resp.encoding)\n",
    "    \n",
    "driver.get(test_url)\n",
    "\n",
    "soup=BeautifulSoup(driver.page_source)\n",
    "\n",
    "#print(soup)\n",
    "\n",
    "job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "print(job_title.text)\n",
    "\n",
    "\n",
    "company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "print(company.text)\n",
    "\n",
    "location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "print(location.text)\n",
    "\n",
    "job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "print(job_description.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor=collection.find()\n",
    "\n",
    "for doc in cursor:\n",
    "    print(doc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#by setting jobs=[] as a list at the beginning and then append dictionarys into it you create a list with jobs items.\n",
    "jobs=[]\n",
    "\n",
    "for i in range(len(urls)): #change it back to for i in range(len(urls)): \n",
    "    get_info = True\n",
    "    try:\n",
    "        driver.get(base_url+urls[i]) #The driver.get method will navigate to a page given by the URL.\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,2200)/1000.0\n",
    "    \n",
    "    \n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source)\n",
    "        print(soup)\n",
    "        \n",
    "        #head_tag=soup.head.title\n",
    "        \n",
    "        #title_tag = head_tag.contents[0]\n",
    "        \n",
    "        #print(title_tag)\n",
    "        \n",
    "        \n",
    "        #job_title=soup.b.string\n",
    "        \n",
    "        job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "        #print(job_title.text)\n",
    "        \n",
    "    \n",
    "        company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "        #print(company.text)\n",
    "        \n",
    "        location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "        #print(location.text)\n",
    "        \n",
    "        job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "       \n",
    "        #print(job_description.text)\n",
    "\n",
    "        #print (driver.current_url)\n",
    "        \n",
    "        jobs.append({\n",
    "            \n",
    "            \"Job Title\":job_title.text,\n",
    "            \"Company Name\":company.text,\n",
    "            \"Location\":location.text,\n",
    "            \"Url\":driver.current_url,\n",
    "            \"Job Description\":job_description.text\n",
    "            \n",
    "        })\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",'w') as f:\n",
    "    json.dump(jobs,f)\n",
    "        #collection.insert(jobs)\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",\"r\") as page:\n",
    "\n",
    "    #print(\"<>\".format(page.read()))\n",
    "    #page.seek(0) # for error \"JSONDecodeError: Expecting value: line 1 column 1 (char 0)\"\n",
    "    #data=json.load(page)\n",
    "    #print(data)\n",
    "\n",
    "    parsed=json.load(page)\n",
    "\n",
    "\n",
    "    collection.insert(parsed)\n",
    "\n",
    "\n",
    "\n",
    "pprint.pprint(collection.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#by setting jobs=[] as a list at the beginning and then append dictionarys into it you create a list with jobs items.\n",
    "jobs=[]\n",
    "\n",
    "for i in range(len(urls)): #change it back to for i in range(len(urls)): \n",
    "    get_info = True\n",
    "    try:\n",
    "        driver.get(base_url+urls[i]) #The driver.get method will navigate to a page given by the URL.\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,2200)/1000.0\n",
    "    \n",
    "    \n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source)\n",
    "        print(soup)\n",
    "        \n",
    "        #head_tag=soup.head.title\n",
    "        \n",
    "        #title_tag = head_tag.contents[0]\n",
    "        \n",
    "        #print(title_tag)\n",
    "        \n",
    "        \n",
    "        #job_title=soup.b.string\n",
    "        \n",
    "        job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "        #print(job_title.text)\n",
    "        \n",
    "    \n",
    "        company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "        #print(company.text)\n",
    "        \n",
    "        location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "        #print(location.text)\n",
    "        \n",
    "        job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "       \n",
    "        #print(job_description.text)\n",
    "\n",
    "        #print (driver.current_url)\n",
    "        \n",
    "        jobs.append({\n",
    "            \n",
    "            \"Job Title\":job_title.text,\n",
    "            \"Company Name\":company.text,\n",
    "            \"Location\":location.text,\n",
    "            \"Url\":driver.current_url,\n",
    "            \"Job Description\":job_description.text\n",
    "            \n",
    "        })\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",'w') as f:\n",
    "    json.dump(jobs,f)\n",
    "        #collection.insert(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The commented out lines \"page.seek(0) \"can fix the error: JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",\"r\") as page:\n",
    "\n",
    "#print(\"<>\".format(page.read()))\n",
    "#page.seek(0) # for error \"JSONDecodeError: Expecting value: line 1 column 1 (char 0)\"\n",
    "#data=json.load(page)\n",
    "#print(data)\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
