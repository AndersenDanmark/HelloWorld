{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from goose3 import Goose\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import pprint\n",
    "from flask import Flask\n",
    "from flask import send_from_directory\n",
    "from flask import request\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat an object called jobs to dump scraped job descriptions to it\n",
    "jobs={}\n",
    "job_keywords=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_languages=['bash','r','python','java','c++','ruby','perl','matlab','javascript','scala','php']\n",
    "analysis_software=['excel','tableau','d3.js','sas','spss','d3','saas','pandas','numpy','scipy','sps','spotfire','scikits.learn','splunk','powerpoint','h2o']\n",
    "bigdata_tool=['hadoop','mapreduce','spark','pig','hive','shark','oozie','zookeeper','flume','mahout']\n",
    "databases=['sql','nosql','hbase','cassandra','mongodb','mysql','mssql','postgresql','oracle db','rdbms']\n",
    "overall_dict = program_languages + analysis_software + bigdata_tool + databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following two functions are for webpage text processing to extract the skill keywords.    \n",
    "def keywords_extract(url):\n",
    "    g = Goose()\n",
    "    article = g.extract(url=url)\n",
    "    text = article.cleaned_text\n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text) #get rid of things that aren't words; 3 for d3 and + for c++\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\")) #filter out stop words in english language\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = list(set(text))\n",
    "    keywords = [str(word) for word in text if word in overall_dict]\n",
    "    return keywords\n",
    "\n",
    "#for this function, thanks to this blog:https://jessesw.com/Data-Science-Skills/    \n",
    "def keywords_f(soup_obj):\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    text = soup_obj.get_text() \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into line\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    text = ''.join(chunk for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "    try:\n",
    "        text = text.decode('utf-8') \n",
    "        #text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                          \n",
    "        return                                                       \n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text)  \n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text) # Fix spacing issue from merged words\n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = list(set(text)) #only care about if a word appears, don't care about the frequency\n",
    "    keywords = [str(word) for word in text if word in overall_dict] #if a skill keyword is found, return it.\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prevent the driver stopping due to the unexpectedAlertBehaviour.# preven \n",
    "webdriver.DesiredCapabilities.FIREFOX[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "get_info = True\n",
    "\n",
    "DRIVER_EXE = r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\data_skills\\geckodriver.exe\"\n",
    "#the instance of Firefox WebDriver is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent the driver stopping due to the unexpectedAlertBehaviour.\n",
    "webdriver.DesiredCapabilities.FIREFOX[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "get_info = True\n",
    "\n",
    "DRIVER_EXE = r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\data_skills\\geckodriver.exe\"\n",
    "#the instance of Firefox WebDriver is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build connection to MongoDB in Heroku\n",
    "#client = MongoClient('mongodb://127.0.0.1:27017')\n",
    "\n",
    "client =MongoClient(\"mongodb://andrewyan:andrewyan!23@ds237660.mlab.com:37660/heroku_zdtgskz7\")\n",
    "\n",
    "#creating a database called jobs_database\n",
    "db = client.heroku_zdtgskz7\n",
    "\n",
    "#creating a collection (table) called collection\n",
    "collection = db.datasciencejobs\n",
    "\n",
    "#job_id = db.collection.insert_one(jobs).inserted_id\n",
    "#collection.find_one({\"Job Title\": \"Data Scientist\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ca.indeed.com/jobs?q=%22web+developer%22&l=Calgary,AB&fromage=Any&limit=10&sort=date&start=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#to change arguments inside URLs \n",
    "\n",
    "base_url = \"https://ca.indeed.com/jobs\" \n",
    "\n",
    "original_url = \"http://www.indeed.com/\"\n",
    "\n",
    "job_search_keywords='\"web developer\"'\n",
    "\n",
    "\n",
    "#job_search_keywords='\"fullstack\" or \"full stack\" or \"software\" or \"programmer\" or \"web developer\" or \"data scientist\" or \"software engineer\" or \"web application\" or \"software developer\" or \"java developer\"'\n",
    "\n",
    "job_search_sortbydate=\"date\" #indeed's default sort method is relevance\n",
    "\n",
    "job_search_location='Calgary,AB'\n",
    "\n",
    "# fromage value below, is age of jobs published,values are 'Any' or '15' or '7' or '3' or '1'\n",
    "job_on_market_days='Any'\n",
    "\n",
    "#limit value below, is the number of jobs to be displayed per page, values are \"10\", \"20\", \"30\", \"50\"\n",
    "job_result_per_page=10\n",
    "\n",
    "search_result_pg=0 #search_result_pg=20 is the 3rd page, the 1st page value is 0, 2nd page value is 10.\n",
    "\n",
    "mydict={'q':job_search_keywords,\n",
    "       'l':job_search_location,\n",
    "        'fromage':job_on_market_days,\n",
    "        'limit':job_result_per_page,        \n",
    "        'sort':job_search_sortbydate,\n",
    "       'start':search_result_pg}\n",
    "\n",
    "resp_make_url=requests.get(base_url,params=mydict)\n",
    "\n",
    "searched_url=resp_make_url.url\n",
    "\n",
    "print(searched_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Firefox\n",
    "driver=webdriver.Firefox(executable_path=DRIVER_EXE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file c:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # of jobs on this page is :13\n",
      "['sql', 'java'] 1\n",
      "https://www.indeed.com/cmp/Servall-Development/jobs/Full-Stack-Developer-0d9a2189fceb34e0?vjs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:126: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Job ID': '0d9a2189fceb34e0', 'Job Title': 'Full Stack Developer', 'Company Name': 'Servall Development', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/Servall-Development/jobs/Full-Stack-Developer-0d9a2189fceb34e0?vjs=3', 'Job Skills': ['sql', 'java']}\n",
      "['sas', 'php', 'java'] 1\n",
      "https://www.indeed.com/cmp/Mobility-Quotient-Solutions-Inc./jobs/Front-End-Web-Developer-158d5535b4c0be62?vjs=3\n",
      "{'Job ID': '158d5535b4c0be62', 'Job Title': 'Front End Web Developer', 'Company Name': 'Mobility Quotient Solutions Inc.', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/Mobility-Quotient-Solutions-Inc./jobs/Front-End-Web-Developer-158d5535b4c0be62?vjs=3', 'Job Skills': ['sas', 'php', 'java']}\n",
      "['javascript', 'java'] 1\n",
      "https://www.indeed.com/viewjob?jk=2b6c4cc725eb2370&from=serp&vjs=3\n",
      "{'Job ID': '2b6c4cc725eb2370', 'Job Title': 'Front-End Web Developer', 'Company Name': 'Autodata Solutions', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/viewjob?jk=2b6c4cc725eb2370&from=serp&vjs=3', 'Job Skills': ['javascript', 'java']}\n",
      "['javascript', 'ruby', 'python', 'php', 'java'] 1\n",
      "https://www.indeed.com/cmp/Arcurve-Inc./jobs/Full-Stack-Web-Developer-5a4b3de9fadd8bac?vjs=3\n",
      "{'Job ID': '5a4b3de9fadd8bac', 'Job Title': 'Full Stack Web Developer', 'Company Name': 'Arcurve Inc.', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/Arcurve-Inc./jobs/Full-Stack-Web-Developer-5a4b3de9fadd8bac?vjs=3', 'Job Skills': ['javascript', 'ruby', 'python', 'php', 'java']}\n",
      "['javascript', 'sql'] 1\n",
      "https://www.indeed.com/cmp/Dynamysk/jobs/Junior-Intermediate-Web-Developer-fdd4098b6e86fa5f?vjs=3\n",
      "{'Job ID': 'fdd4098b6e86fa5f', 'Job Title': 'Junior/Intermediate Web Developer', 'Company Name': 'Dynamysk', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/Dynamysk/jobs/Junior-Intermediate-Web-Developer-fdd4098b6e86fa5f?vjs=3', 'Job Skills': ['javascript', 'sql']}\n",
      "['javascript', 'java'] 1\n",
      "https://www.indeed.com/cmp/Appropolis-Inc./jobs/Web-Developer-fc8702ae43a27c1f?vjs=3\n",
      "{'Job ID': 'fc8702ae43a27c1f', 'Job Title': 'Web developer (Angular or React)', 'Company Name': 'Appropolis Inc.', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/Appropolis-Inc./jobs/Web-Developer-fc8702ae43a27c1f?vjs=3', 'Job Skills': ['javascript', 'java']}\n",
      "['r', 'sql', 'java'] 1\n",
      "https://www.indeed.com/cmp/Noralta-Technologies-Inc./jobs/Senior-Web-Developer-ff42b99049c06b7e?vjs=3\n",
      "{'Job ID': 'ff42b99049c06b7e', 'Job Title': 'Senior Web Developer', 'Company Name': 'Noralta Technologies Inc.', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/Noralta-Technologies-Inc./jobs/Senior-Web-Developer-ff42b99049c06b7e?vjs=3', 'Job Skills': ['r', 'sql', 'java']}\n",
      "['php'] 1\n",
      "https://www.indeed.com/cmp/KRD-Consulting-Group/jobs/Web-Developer-a579fbe19dc002a5?vjs=3\n",
      "{'Job ID': 'a579fbe19dc002a5', 'Job Title': 'Web Developer - Summer Student', 'Company Name': 'KRD Consulting Group', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/KRD-Consulting-Group/jobs/Web-Developer-a579fbe19dc002a5?vjs=3', 'Job Skills': ['php']}\n",
      "[] 1\n",
      "https://www.indeed.com/cmp/4iiii-Innovations-Inc./jobs/Front-End-Web-Developer-bb9e7cef17ad2ea7?vjs=3\n",
      "{'Job ID': 'bb9e7cef17ad2ea7', 'Job Title': 'Front-End Web Developer', 'Company Name': '4iiii Innovations Inc.', 'Location': 'Cochrane, AB', 'Url': 'https://www.indeed.com/cmp/4iiii-Innovations-Inc./jobs/Front-End-Web-Developer-bb9e7cef17ad2ea7?vjs=3', 'Job Skills': []}\n",
      "[] 1\n",
      "https://www.indeed.com/cmp/TourneyHub/jobs/Founder-e12510a6c17d9d27?vjs=3\n",
      "{'Job ID': 'e12510a6c17d9d27', 'Job Title': 'Co-founder: Web Developer', 'Company Name': 'TourneyHub', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/TourneyHub/jobs/Founder-e12510a6c17d9d27?vjs=3', 'Job Skills': []}\n",
      "['java'] 1\n",
      "https://www.indeed.com/cmp/ZGM-Collaborative-Marketing/jobs/Front-End-Web-Developer-e3e972e61e23d0d8?sjdu=vQIlM60yK_PwYat7ToXhk_OGBUDcSLAVanATIv7U07QkCDw8zOhkeKPRPEjQCLkYntcr6rNsNKgIU53yf8mES-z0zeSTdp_UDdj5syYlXWw&vjs=3\n",
      "{'Job ID': 'e3e972e61e23d0d8', 'Job Title': 'Front-End Web Developer', 'Company Name': 'ZGM Modern Marketing Partners', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/ZGM-Collaborative-Marketing/jobs/Front-End-Web-Developer-e3e972e61e23d0d8?sjdu=vQIlM60yK_PwYat7ToXhk_OGBUDcSLAVanATIv7U07QkCDw8zOhkeKPRPEjQCLkYntcr6rNsNKgIU53yf8mES-z0zeSTdp_UDdj5syYlXWw&vjs=3', 'Job Skills': ['java']}\n",
      "['python', 'php', 'sql', 'java'] 1\n",
      "https://www.indeed.com/cmp/UNITED-DECORATING-INC/jobs/Web-Developer-c4b3103f23b036ff?sjdu=Zzi_VW2ygsY1fzh3Ma9ZsE4zIT1NTXCwgFBhdjeTC3PE9rHzrJP6SJUOom1CpbSZEA3ejbNMRv4CDMon0Ceqsw&vjs=3\n",
      "{'Job ID': 'c4b3103f23b036ff', 'Job Title': 'Web Developer', 'Company Name': 'United Decorating Inc.', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/UNITED-DECORATING-INC/jobs/Web-Developer-c4b3103f23b036ff?sjdu=Zzi_VW2ygsY1fzh3Ma9ZsE4zIT1NTXCwgFBhdjeTC3PE9rHzrJP6SJUOom1CpbSZEA3ejbNMRv4CDMon0Ceqsw&vjs=3', 'Job Skills': ['python', 'php', 'sql', 'java']}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-3d02bebf6fe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#waits for a random time so that the website don't consider you as a bot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test\n",
    "page_number=0\n",
    "\n",
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#change the rage to change number of search result pages to iterate\n",
    "\n",
    "for page_number in range(0,1):\n",
    "    \n",
    "    mydict={'q':job_search_keywords,\n",
    "           'l':job_search_location,\n",
    "            'fromage':job_on_market_days,\n",
    "            'limit':job_result_per_page,        \n",
    "            'sort':job_search_sortbydate,\n",
    "           'start':search_result_pg}\n",
    "\n",
    "    resp_make_url=requests.get(base_url,params=mydict)\n",
    "    \n",
    "    searched_url=resp_make_url.url\n",
    "    \n",
    "    \n",
    "#loop through one page of searched results:\n",
    "    \n",
    "    #creating a response object called resp\n",
    "    resp = requests.get(searched_url)\n",
    "\n",
    "    #You can find out what encoding Requests is using, and change it, using the r.encoding property\n",
    "    #print(resp.encoding)\n",
    "\n",
    "    # Running the url link through BeautifulSoup give us a BeautifulSoup object, which represents the document as a nested data structure.\n",
    "    start_soup = BeautifulSoup(resp.content)\n",
    "    urls = start_soup.findAll('a',{'rel':'nofollow','target':'_blank'}) #this are the links of the job posts\n",
    "\n",
    "    urls = [link['href'] for link in urls] \n",
    "    \n",
    "    #The line above is equivalent to below:\n",
    "    \n",
    "    #To store all url links to a list called links.\n",
    "        #urls=[]\n",
    "        #for link in urls:\n",
    "        #    urls.append(link.get('href'))\n",
    "        \n",
    "        #print(urls[0:2])\n",
    "  \n",
    "    \n",
    "    #print job links of the current search result page:\n",
    "    print(\"total # of jobs on this page is :\"+str(len(urls)))\n",
    "\n",
    "    \n",
    "\n",
    "       \n",
    "    for url in urls: #change it back to for i in range(len(urls)): \n",
    "        get_info = True\n",
    "        try:\n",
    "            driver.get(original_url+url) #The driver.get method will navigate to a page given by the URL.\n",
    "        except TimeoutException:\n",
    "            get_info = False\n",
    "        except IndexError:\n",
    "            get_info = False\n",
    "            continue\n",
    "        j = random.randint(1500,3200)/1000.0 #change these values to avoid anti-scraping\n",
    "\n",
    "\n",
    "        time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "\n",
    "\n",
    "        if get_info:\n",
    "            soup=BeautifulSoup(driver.page_source)\n",
    "            \n",
    "            #print 'extracting %d job keywords...' % i\n",
    "            single_job = keywords_f(soup)\n",
    "            print(single_job,len(soup))\n",
    "            print (driver.current_url)\n",
    "            #job_keywords.append([driver.current_url,single_job])\n",
    "    \n",
    "            \n",
    "            job_id_values=soup.find(\"div\",{\"class\":\"edit_note_content\"})\n",
    "            job_id=job_id_values['id'][11:]\n",
    "            #job_id_values data type is tag, to access a tag's attribute values use tag['attribute_key']\n",
    "            \n",
    "\n",
    "            '''\n",
    "            print(\"job_id_values  is:\")\n",
    "            print(job_id_values)\n",
    "            print(job_id_values['id'])\n",
    "            print(\"job_id_values type is:\")\n",
    "            print(type(job_id_values))\n",
    "            print(\"job_id is:\")\n",
    "            print(job_id)\n",
    "            print(\"job_id type is:\")\n",
    "            print(type(job_id))\n",
    "            '''\n",
    "\n",
    "           \n",
    "            job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "            #print(job_title.text)\n",
    "\n",
    "\n",
    "            company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "            #print(company.text)\n",
    "\n",
    "            location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "            #print(location.text)\n",
    "\n",
    "            job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "            \n",
    "            \n",
    "\n",
    "            #print(job_description.text)\n",
    "\n",
    "            #print (driver.current_url)\n",
    "\n",
    "            jobs={\n",
    "                \"Job ID\":job_id,\n",
    "                \"Job Title\":job_title.text,\n",
    "                \"Company Name\":company.text,\n",
    "                \"Location\":location.text,\n",
    "                \"Url\":driver.current_url,\n",
    "                \"Job Skills\":single_job,\n",
    "                \"Job Description\":job_description.text\n",
    "\n",
    "            }\n",
    "            #This line inserts each job into MongoDB\n",
    "            #pprint.pprint(jobs)\n",
    "            collection.update({\"Job ID\":job_id},jobs,upsert=True)\n",
    "            \n",
    "            print(jobs)\n",
    "         \n",
    "    \n",
    "#move to the next page of searched results, after iterating through all jobs links in current page\n",
    "    search_result_pg+=len(urls)\n",
    "    \n",
    "    #print(page_number)\n",
    "    \n",
    "    #print(searched_url) # print urls for search result page 1, 2 ,3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test a webpage\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "driver=webdriver.Firefox(executable_path=DRIVER_EXE)\n",
    "\n",
    "#Enter the url below\n",
    "\n",
    "test_url=\"https://www.indeed.com/rc/clk?jk=2188071b67e80bef&fccid=2b585dc0d549b736&vjs=3\"\n",
    "#test_url=\"https://www.indeed.com/viewjob?jk=fb2e3433eade4b6c&tk=1cf6na4o9btcr9g6&from=serp&alid=3&advn=8231918522092453\"\n",
    "    \n",
    "    \n",
    "#resp = requests.get(test_url)\n",
    "\n",
    "    #You can find out what encoding Requests is using, and change it, using the r.encoding property\n",
    "    #print(resp.encoding)\n",
    "    \n",
    "driver.get(test_url)\n",
    "\n",
    "soup=BeautifulSoup(driver.page_source)\n",
    "\n",
    "#print(soup)\n",
    "\n",
    "job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "print(job_title.text)\n",
    "\n",
    "\n",
    "company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "print(company.text)\n",
    "\n",
    "location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "print(location.text)\n",
    "\n",
    "job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "print(job_description.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor=collection.find()\n",
    "\n",
    "for doc in cursor:\n",
    "    print(doc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#by setting jobs=[] as a list at the beginning and then append dictionarys into it you create a list with jobs items.\n",
    "jobs=[]\n",
    "\n",
    "for i in range(len(urls)): #change it back to for i in range(len(urls)): \n",
    "    get_info = True\n",
    "    try:\n",
    "        driver.get(base_url+urls[i]) #The driver.get method will navigate to a page given by the URL.\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,2200)/1000.0\n",
    "    \n",
    "    \n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source)\n",
    "        print(soup)\n",
    "        \n",
    "        #head_tag=soup.head.title\n",
    "        \n",
    "        #title_tag = head_tag.contents[0]\n",
    "        \n",
    "        #print(title_tag)\n",
    "        \n",
    "        \n",
    "        #job_title=soup.b.string\n",
    "        \n",
    "        job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "        #print(job_title.text)\n",
    "        \n",
    "    \n",
    "        company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "        #print(company.text)\n",
    "        \n",
    "        location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "        #print(location.text)\n",
    "        \n",
    "        job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "       \n",
    "        #print(job_description.text)\n",
    "\n",
    "        #print (driver.current_url)\n",
    "        \n",
    "        jobs.append({\n",
    "            \n",
    "            \"Job Title\":job_title.text,\n",
    "            \"Company Name\":company.text,\n",
    "            \"Location\":location.text,\n",
    "            \"Url\":driver.current_url,\n",
    "            \"Job Description\":job_description.text\n",
    "            \n",
    "        })\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",'w') as f:\n",
    "    json.dump(jobs,f)\n",
    "        #collection.insert(jobs)\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",\"r\") as page:\n",
    "\n",
    "    #print(\"<>\".format(page.read()))\n",
    "    #page.seek(0) # for error \"JSONDecodeError: Expecting value: line 1 column 1 (char 0)\"\n",
    "    #data=json.load(page)\n",
    "    #print(data)\n",
    "\n",
    "    parsed=json.load(page)\n",
    "\n",
    "\n",
    "    collection.insert(parsed)\n",
    "\n",
    "\n",
    "\n",
    "pprint.pprint(collection.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#by setting jobs=[] as a list at the beginning and then append dictionarys into it you create a list with jobs items.\n",
    "jobs=[]\n",
    "\n",
    "for i in range(len(urls)): #change it back to for i in range(len(urls)): \n",
    "    get_info = True\n",
    "    try:\n",
    "        driver.get(base_url+urls[i]) #The driver.get method will navigate to a page given by the URL.\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,2200)/1000.0\n",
    "    \n",
    "    \n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source)\n",
    "        print(soup)\n",
    "        \n",
    "        #head_tag=soup.head.title\n",
    "        \n",
    "        #title_tag = head_tag.contents[0]\n",
    "        \n",
    "        #print(title_tag)\n",
    "        \n",
    "        \n",
    "        #job_title=soup.b.string\n",
    "        \n",
    "        job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "        #print(job_title.text)\n",
    "        \n",
    "    \n",
    "        company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "        #print(company.text)\n",
    "        \n",
    "        location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "        #print(location.text)\n",
    "        \n",
    "        job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "       \n",
    "        #print(job_description.text)\n",
    "\n",
    "        #print (driver.current_url)\n",
    "        \n",
    "        jobs.append({\n",
    "            \n",
    "            \"Job Title\":job_title.text,\n",
    "            \"Company Name\":company.text,\n",
    "            \"Location\":location.text,\n",
    "            \"Url\":driver.current_url,\n",
    "            \"Job Description\":job_description.text\n",
    "            \n",
    "        })\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",'w') as f:\n",
    "    json.dump(jobs,f)\n",
    "        #collection.insert(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The commented out lines \"page.seek(0) \"can fix the error: JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",\"r\") as page:\n",
    "\n",
    "#print(\"<>\".format(page.read()))\n",
    "#page.seek(0) # for error \"JSONDecodeError: Expecting value: line 1 column 1 (char 0)\"\n",
    "#data=json.load(page)\n",
    "#print(data)\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
