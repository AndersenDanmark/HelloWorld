{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from goose3 import Goose\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import pprint\n",
    "from flask import Flask\n",
    "from flask import send_from_directory\n",
    "from flask import request\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat an object called jobs to dump scraped job descriptions to it\n",
    "jobs={}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prevent the driver stopping due to the unexpectedAlertBehaviour.# preven \n",
    "webdriver.DesiredCapabilities.FIREFOX[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "get_info = True\n",
    "\n",
    "DRIVER_EXE = r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\data_skills\\geckodriver.exe\"\n",
    "#the instance of Firefox WebDriver is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent the driver stopping due to the unexpectedAlertBehaviour.\n",
    "webdriver.DesiredCapabilities.FIREFOX[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "get_info = True\n",
    "\n",
    "DRIVER_EXE = r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\data_skills\\geckodriver.exe\"\n",
    "#the instance of Firefox WebDriver is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build connection\n",
    "#client = MongoClient('mongodb://127.0.0.1:27017')\n",
    "\n",
    "client =MongoClient(\"mongodb://andrewyan:andrewyan!23@ds237660.mlab.com:37660/heroku_zdtgskz7\")\n",
    "\n",
    "#creating a database called jobs_database\n",
    "db = client.heroku_zdtgskz7\n",
    "\n",
    "#creating a collection (table) called collection\n",
    "collection = db.datasciencejobs\n",
    "\n",
    "#job_id = db.collection.insert_one(jobs).inserted_id\n",
    "#collection.find_one({\"Job Title\": \"Data Scientist\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ca.indeed.com/jobs?q=developer&l=Calgary,AB&fromage=Any&limit=10&sort=date&start=0\n"
     ]
    }
   ],
   "source": [
    "#to change arguments inside URLs\n",
    "\n",
    "base_url = \"https://ca.indeed.com/jobs\" \n",
    "\n",
    "original_url = \"http://www.indeed.com/\"\n",
    "\n",
    "job_search_keywords=\"developer\"\n",
    "\n",
    "job_search_sortbydate=\"date\" #indeed's default sort method is relevance\n",
    "\n",
    "job_search_location='Calgary,AB'\n",
    "\n",
    "# fromage value below, is age of jobs published,values are 'Any' or '15' or '7' or '3' or '1'\n",
    "job_on_market_days='Any'\n",
    "\n",
    "#limit value below, is the number of jobs to be displayed per page, values are \"10\", \"20\", \"30\", \"50\"\n",
    "job_result_per_page=10\n",
    "\n",
    "search_result_pg=0 #search_result_pg=20 is the 3rd page, the 1st page value is 0, 2nd page value is 10.\n",
    "\n",
    "mydict={'q':job_search_keywords,\n",
    "       'l':job_search_location,\n",
    "        'fromage':job_on_market_days,\n",
    "        'limit':job_result_per_page,        \n",
    "        'sort':job_search_sortbydate,\n",
    "       'start':search_result_pg}\n",
    "\n",
    "resp_make_url=requests.get(base_url,params=mydict)\n",
    "\n",
    "searched_url=resp_make_url.url\n",
    "\n",
    "print(searched_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ca.indeed.com/jobs?q=%22software+developer%22&l=Calgary,AB&fromage=Any&limit=10&sort=date&start=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#to change arguments inside URLs#to chan \n",
    "\n",
    "base_url = \"https://ca.indeed.com/jobs\" \n",
    "\n",
    "original_url = \"http://www.indeed.com/\"\n",
    "\n",
    "job_search_keywords='\"software developer\"'\n",
    "\n",
    "\n",
    "#job_search_keywords='\"fullstack\" or \"full stack\" or \"software\" or \"programmer\" or \"web developer\" or \"data scientist\" or \"software engineer\" or \"web application\" or \"software developer\" or \"java developer\"'\n",
    "\n",
    "job_search_sortbydate=\"date\" #indeed's default sort method is relevance\n",
    "\n",
    "job_search_location='Calgary,AB'\n",
    "\n",
    "# fromage value below, is age of jobs published,values are 'Any' or '15' or '7' or '3' or '1'\n",
    "job_on_market_days='Any'\n",
    "\n",
    "#limit value below, is the number of jobs to be displayed per page, values are \"10\", \"20\", \"30\", \"50\"\n",
    "job_result_per_page=10\n",
    "\n",
    "search_result_pg=0 #search_result_pg=20 is the 3rd page, the 1st page value is 0, 2nd page value is 10.\n",
    "\n",
    "mydict={'q':job_search_keywords,\n",
    "       'l':job_search_location,\n",
    "        'fromage':job_on_market_days,\n",
    "        'limit':job_result_per_page,        \n",
    "        'sort':job_search_sortbydate,\n",
    "       'start':search_result_pg}\n",
    "\n",
    "resp_make_url=requests.get(base_url,params=mydict)\n",
    "\n",
    "searched_url=resp_make_url.url\n",
    "\n",
    "print(searched_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Firefox\n",
    "driver=webdriver.Firefox(executable_path=DRIVER_EXE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "css_soup is : \n",
      "<html><body><p class=\"body\"></p></body></html>\n",
      "\n",
      "\n",
      "css_soup data type is : \n",
      "<class 'bs4.BeautifulSoup'>\n",
      "\n",
      "\n",
      "css_soup.p is : \n",
      "<p class=\"body\"></p>\n",
      "\n",
      "\n",
      "css_soup.p data type is : \n",
      "<class 'bs4.element.Tag'>\n",
      "['body']\n",
      "\n",
      "\n",
      "job_id_values  is:\n",
      "<html><body><div class=\"edit_note_content\" id=\"editsaved2_6b402e29a2c5c49a\" style=\"display:none;\"></div></body></html>\n",
      "job_id_values type is:\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "\n",
      "\n",
      "job_id_values.div  is:\n",
      "<div class=\"edit_note_content\" id=\"editsaved2_6b402e29a2c5c49a\" style=\"display:none;\"></div>\n",
      "job_id_values.div type is:\n",
      "<class 'bs4.element.Tag'>\n",
      "\n",
      "\n",
      "job_id is:\n",
      "6b402e29a2c5c49a\n",
      "job_id type is:\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file c:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "There are 4 kinds of objects in Beautifulsoup:  Tag, NavigableString, BeautifulSoup, and Comment.\n",
    "\n",
    "BeautifulSoup contains tag, NavigableString, and Comment\n",
    "\n",
    "To access tags inside BeautifulSoup, treat tags as an instance inside BeautifulSoup. \n",
    "\n",
    "For example: BeautifulSoup.tag\n",
    "\n",
    "Tags have attributes, which contain keys and values.\n",
    "To Access them, treat tags as dictionaries.\n",
    "\n",
    "tag['key']\n",
    "\n",
    "To access values inside an object, first try to understand what data type you object belongs to, then find the correct methods to access the values.\n",
    "\n",
    "'''\n",
    "#How to get an attribute's value from a Beautifulsoup tag\n",
    "\n",
    "css_soup = BeautifulSoup('<p class=\"body\"></p>')\n",
    "attribute_value=css_soup.p['class']\n",
    "\n",
    "print(\"css_soup is : \")\n",
    "print(css_soup)\n",
    "print('\\n')\n",
    "\n",
    "print(\"css_soup data type is : \")\n",
    "print(type(css_soup))\n",
    "print('\\n')\n",
    "\n",
    "print(\"css_soup.p is : \")\n",
    "print(css_soup.p)\n",
    "print('\\n')\n",
    "\n",
    "print(\"css_soup.p data type is : \")\n",
    "print(type(css_soup.p))\n",
    "print(attribute_value)\n",
    "print('\\n')\n",
    "\n",
    "job_id_values=BeautifulSoup('<div class=\"edit_note_content\" id=\"editsaved2_6b402e29a2c5c49a\" style=\"display:none;\"></div>')\n",
    "job_id=job_id_values.div['id'][11:]\n",
    "#job_id_values data type is tag, to access a tag's attribute values use tag['attribute_key']\n",
    "\n",
    "\n",
    "print(\"job_id_values  is:\")\n",
    "print(job_id_values)\n",
    "\n",
    "print(\"job_id_values type is:\")\n",
    "print(type(job_id_values))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"job_id_values.div  is:\")\n",
    "print(job_id_values.div)\n",
    "\n",
    "print(\"job_id_values.div type is:\")\n",
    "print(type(job_id_values.div))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"job_id is:\")\n",
    "print(job_id)\n",
    "print(\"job_id type is:\")\n",
    "print(type(job_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file c:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # of jobs on this page is :13\n",
      "job_id_values  is:\n",
      "<div class=\"edit_note_content\" id=\"editsaved2_6b402e29a2c5c49a\" style=\"display:none;\"></div>\n",
      "editsaved2_6b402e29a2c5c49a\n",
      "job_id_values type is:\n",
      "<class 'bs4.element.Tag'>\n",
      "job_id is:\n",
      "6b402e29a2c5c49a\n",
      "job_id type is:\n",
      "<class 'str'>\n",
      "Senior Software Developer\n",
      "Shaw Communications\n",
      "Calgary, AB\n",
      "{'Job ID': '6b402e29a2c5c49a', 'Job Title': 'Senior Software Developer', 'Company Name': 'Shaw Communications', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/viewjob?jk=6b402e29a2c5c49a&from=serp&vjs=3', 'Job Description': 'Every day, our passionate employees connect our customers to the world and everything in it – it’s why we exist at Shaw, and it’s why we strive to be the place where the best people choose to work. Our people reflect the richness of our communities, and our culture is inclusive of each individual’s diverse background and perspective, which makes us a stronger team.\\n\\nAt Shaw we believe a diverse workforce fosters diversity of thought and perspective, and more diversity means more solutions. We invite all qualified individuals to apply.\\n\\nCareer Opportunity: Senior Software Developer, Inventory Systems\\n\\nAt Shaw Communications, we have an exciting permanent full time opportunity for a Senior Software Developer based out of our Calgary office. This role will be instrumental in the implementation and development of our logical and spatial inventory systems within the Operational Support Systems (OSS) domain.\\n\\nAccountabilities/ Responsibilities:\\nDeliver quality solutions that meet business requirements and align with long-term OSS architecture and roadmapsDevelop and design API’s to interact with inventory dataDevelop and design discovery and reconciliation workflowsDevelop Data Models to support network devices and servicesEngage and maintain a healthy working relationship with our internal customers, partners and vendorsEngage as a member of project teams in order to deliver successfullyParticipate in design, delivery and warranty of a projectKeeps up to date on current delivery methodologies for software and systemsKeeps up to date on technology in the industryParticipate in code reviews and mentoring\\nRequired Skills, Experience and Qualifications:\\nBachelor’s degree or equivalent post-secondary education in Computer ScienceAt least 10 years of experience with Java Software or other languages3 or more years of industry experience in cable or telecom would be beneficialExperience with spatial and logical inventory systems (Netcracker, SpatialNet, data center information systems, or other inventory systems)A proven track record of developing and implementing solution architectures for complex user-facing systemsProvide technical thought leadership while working closely with clients having both technical and non-technical backgroundsExcellent communications skillsExperience using agile and/or lean methodologies\\nIf you want to be part of something bigger—part of a dynamic team of people who are aligned, focused, and passionate about working together to care for our customers, then we look forward to meeting you! Join us.\\n\\nWe thank all interested applicants; however, only qualified candidates will be contacted. This position requires the successful completion of a criminal and credit background check.\\n\\nShaw Communications, through its third party vendor, collects and stores information applicable to the candidate profile you create when you submit the information asked for below. The use and disclosure of the collected information is for the sole purpose of job search and placement activities for Shaw Communications. The information is subject to Personal Information Protection and Electronic Documents Act. The information will be retained and when disposed of, it is done so in a secure manner. Your profile will become inactive and moved to the archive if you do not access your profile for a period of 1 year.'}\n",
      "job_id_values  is:\n",
      "<div class=\"edit_note_content\" id=\"editsaved2_78bc725c6bc3abdd\" style=\"display:none;\"></div>\n",
      "editsaved2_78bc725c6bc3abdd\n",
      "job_id_values type is:\n",
      "<class 'bs4.element.Tag'>\n",
      "job_id is:\n",
      "78bc725c6bc3abdd\n",
      "job_id type is:\n",
      "<class 'str'>\n",
      "Software Developer\n",
      "MVP Talent Corp\n",
      "Calgary, AB\n",
      "{'Job ID': '78bc725c6bc3abdd', 'Job Title': 'Software Developer', 'Company Name': 'MVP Talent Corp', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/cmp/MVP-Talent-Corp/jobs/Software-Developer-78bc725c6bc3abdd?vjs=3', 'Job Description': \"Intermediate – Senior Level Developers – Oil and Gas Services – PERMANENT – CalgaryOur client is leading provider of technology comprise of both software hardware (control systems) that is used in Oil and Gas Servicing. There are many open new software development positions open as this is a very busy technology company.We are ideally looking for developers with either C#.Net or Java applications development experience. Our client offers significant career advancement and learning opportunities in a challenging and flexible work environment.This market leader believes people will perform their best when employees are given a chance to make a meaningful contribution, without micro-mgmt and are empowered to do so.MUST HAVE Skills: (preference given to Calgary-based candidates)4-7 years of full-stack software development experience using either C# .net or Java on the back end and HTML5, Javascript, CSS, JQuery, Javascript frameworks (Angular or REACT) on the front end.Hands-on commercial experience developing business n-tier and/or Web-based ApplicationsWeb / Cloud development experienceSQL based Databases or OracleFull stack SDLC and Agile developmentMust have excellent communication and interpersonal skills, youthful passion for software development and eagerness to get ahead / learn and build your dev career.Willing to learn, aptitude for learning new languages and tools (Labview); flexible and accommodating personality.Strong personality, speak-up mentality, debate and challenge for the betterment of softwareCandidates must live in Calgary, AlbertaNice to have skills: Bachelor’s degree in Electrical Engineering, Software / Computer Engineering or Computer ScienceBackground in InstrumentationExperience with additional languages such as Labview, node.jsScope of responsibilityBe able to handle multiple projectMeet project timelines for expected deliverablesResponsible for specific software modules including design, development and testingJob Type: PermanentExperience:HTML5, Javascript, CSS, JQuery, Angular or React: 5 yearsfull-stack including both front and back end dev: 5 yearseither Java or C# .net development: 5 yearsEducation:Bachelor's DegreeLocation:Calgary, AB\"}\n",
      "job_id_values  is:\n",
      "<div class=\"edit_note_content\" id=\"editsaved2_ebf269304db87085\" style=\"display:none;\"></div>\n",
      "editsaved2_ebf269304db87085\n",
      "job_id_values type is:\n",
      "<class 'bs4.element.Tag'>\n",
      "job_id is:\n",
      "ebf269304db87085\n",
      "job_id type is:\n",
      "<class 'str'>\n",
      "Software Engineer III\n",
      "Cisco\n",
      "Calgary, AB\n",
      "{'Job ID': 'ebf269304db87085', 'Job Title': 'Software Engineer III', 'Company Name': 'Cisco', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/viewjob?jk=ebf269304db87085&from=serp&vjs=3', 'Job Description': \"Are you excited by the challenge of protecting people against advanced computer security threats? Do you have the programming skills and experience to improve the advanced detection capabilities of our cloud security platform?\\nWho You’ll Work With\\nBillions of times a day, computers around the world communicate with the Cisco Advanced Malware Protection Cloud and rely on the AMP product to protect them against advanced forms of malware. Some malware is straightforward to identify, but antivirus has done that forever. Our customers need protection against malware that’s tricky to identify:\\nViruses that generate different binaries on every machine they infect\\nThings that seem benign until you start looking closely at their behaviour\\nMalicious programs that have invented new ways of hiding themselves\\nTo make the problem even more interesting: it may only become clear after a few days that a program or behavior was malicious. Does that sound fun to you?\\nThe AMP Data Team is responsible for running advanced malware identification algorithms on incoming event data streams and storing and indexing that data. We index that data both for future detailed investigations of malware incidents, and to retrospectively detect previously unidentified malware in stored data. We strive for sub-second processing latencies in our streaming platforms and databases as they handle data volumes of >100TB/month and growing.\\nDo you love learning and working with technologies like the following?\\nKafka\\nCassandra\\nFlink\\nStorm\\nLinux\\nJRuby\\nIf using those to help improve computer security sounds exciting, this is the place for you.\\nWhat You’ll do\\nOur team is looking for a Software Developer who will help scale our infrastructure as the business grows, make server-side code changes for new features, develop and maintain research tools, and take proof-of-concept detections from our researchers and apply their power to protect our customers.\\nWho You Are\\nYou are self-motivated, results driven and engaged. You’re passionate about back-end development and enjoy collaborating in a team based environment. You have fun learning new technologies. You want to get to the bottom of things, understand what’s going on, and make a difference. You are someone who doesn’t give up when faced with complex problems.\\nMust-haves\\nBachelor’s degree in Computer Science, Math, or Physics\\nEnjoy writing server-side code and unit tests\\nKnowledge of algorithmic complexity\\nAble to debug, diagnose, and resolve occasional production problems\\nHave 2–8 years experience writing object-oriented code and are ok working in Ruby, Java, JRuby, and Scala\\nExperience with Linux command-line and system administration basics: ssh, permissions, packages, log files, &c.\\nNice to have\\n\\nMaster’s degree or PhD\\nExperience detecting malware in event streams\\nExperience with cloud systems architecture\\nAble to smash the stack, use heap sprays, &c., to control EIP\\nReal-world experience with streaming platforms like Storm, Flink, and Kafka Streams\\nProduction experience with distributed databases like Mongo and Cassandra\\nOpen-source contributions\\nPeer-reviewed publications\\nWhy Cisco\\nThe Internet of Everything is a phenomenon driving new opportunities for Cisco and it is transforming our customers' businesses worldwide. We are pioneers and have been since the early days of connectivity. Today, we are building teams that are expanding our technology solutions in the mobile, cloud, security, IT, and big data spaces, including software and consulting services. As Cisco delivers the network that powers the Internet, we are connecting the unconnected. Imagine creating unprecedented disruption. Your revolutionary ideas will influence everything from retail, healthcare, and entertainment, to public and private sectors, and far beyond. Collaborate with like-minded innovators in a fun and flexible culture that has earned Cisco global recognition as a Great Place To Work. With roughly 10 billion connected things in the world now and over 50 billion estimated in the future, your career has exponential possibilities at Cisco.\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status\\n@CiscoCareers #CalgaryJobs #CiscoJobs\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_id_values  is:\n",
      "<div class=\"edit_note_content\" id=\"editsaved2_d730c451898c0308\" style=\"display:none;\"></div>\n",
      "editsaved2_d730c451898c0308\n",
      "job_id_values type is:\n",
      "<class 'bs4.element.Tag'>\n",
      "job_id is:\n",
      "d730c451898c0308\n",
      "job_id type is:\n",
      "<class 'str'>\n",
      "Web Application Software Developer\n",
      "Decisive Farming\n",
      "Calgary, AB\n",
      "{'Job ID': 'd730c451898c0308', 'Job Title': 'Web Application Software Developer', 'Company Name': 'Decisive Farming', 'Location': 'Calgary, AB', 'Url': 'https://www.indeed.com/viewjob?jk=d730c451898c0308&from=serp&vjs=3', 'Job Description': 'Report To: Product Development Manager\\nDepartment: Product Development Team\\nType: Full Time\\nMin. Experience: Four years software / web application development\\nMANDATORY SKILL SETS WITH COMPETENT FLUENCY: C#, ASP.NET 4.5, SQL Server, HTML5, CSS3, JavaScript, Visual Studio\\nBeneficial Skill Sets: GIS technologies, Web Services for native Apps, SSRS\\nDescription:\\nAre you passionate about developing complex, yet responsive web applications for performance, scalability, and exceptional user experience? The Decisive Farming Technology office is seeking a web application software developer to join our development team.\\nThe candidate shall be capable of demonstrating a high level of competency in designing and implementing customer facing web applications that meet business requirements.\\nDuties and Responsibilities:\\nContribute to the analysis of business requirements, prepare design and implementation recommendations and estimate development effort.\\nDesign, develop, test and document the assigned development tasks which include both new features and support activities. Development is primarily using C#, JavaScript and ASP.NET.\\nLead or participate in design reviews, code reviews and architecture evolution discussions.\\nWork as either part of a team or individually as tasks require.\\nContribute to the continuous improvement in development technologies and practices.\\nWork collaboratively and professionally with other staff in cross functional teams to achieve goals.\\nKnowledge and Skill\\nDegree or Diploma in Computer Science, IT / Programming or related field.\\n4+ years of experience as a software / web-application developer using C#, ASP.NET and JavaScript with Visual Studio.\\nExperience with database design, complex query development, and SQL Server Management Studio.\\nAbility to take ownership for assigned tasks and projects, provide development estimates and milestones, and deliver on commitments.\\nExperience with Web Services development supporting native Apps an asset.\\nExperience with GIS technology and mapping applications an asset\\nWhat will set you Apart\\nExperience in the agriculture industry.\\nA strong desire to excel at your tasks.\\nPersonal Qualities\\nExcellent verbal and written communication skills\\nWilling to pitch in wherever necessary\\nCapable of working independently and on a team\\nStrong interest in learning and growing\\nAttention to detail\\nMotivated by quality, excellence and results'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e4ed732e0a18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mget_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_url\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#The driver.get method will navigate to a page given by the URL.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mget_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \"\"\"\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTemplate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstitute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparsed_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m                 \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhttplib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew yan\\appdata\\local\\programs\\python\\python36\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test\n",
    "page_number=0\n",
    "\n",
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#change the rage to change number of search result pages to iterate\n",
    "\n",
    "for page_number in range(0,1):\n",
    "    \n",
    "    mydict={'q':job_search_keywords,\n",
    "           'l':job_search_location,\n",
    "            'fromage':job_on_market_days,\n",
    "            'limit':job_result_per_page,        \n",
    "            'sort':job_search_sortbydate,\n",
    "           'start':search_result_pg}\n",
    "\n",
    "    resp_make_url=requests.get(base_url,params=mydict)\n",
    "    \n",
    "    searched_url=resp_make_url.url\n",
    "    \n",
    "    \n",
    "#loop through one page of searched results:\n",
    "    \n",
    "    #creating a response object called resp\n",
    "    resp = requests.get(searched_url)\n",
    "\n",
    "    #You can find out what encoding Requests is using, and change it, using the r.encoding property\n",
    "    #print(resp.encoding)\n",
    "\n",
    "    # Running the url link through BeautifulSoup give us a BeautifulSoup object, which represents the document as a nested data structure.\n",
    "    start_soup = BeautifulSoup(resp.content)\n",
    "    urls = start_soup.findAll('a',{'rel':'nofollow','target':'_blank'}) #this are the links of the job posts\n",
    "\n",
    "    urls = [link['href'] for link in urls] \n",
    "    \n",
    "    #The line above is equivalent to below:\n",
    "    \n",
    "    #To store all url links to a list called links.\n",
    "        #urls=[]\n",
    "        #for link in urls:\n",
    "        #    urls.append(link.get('href'))\n",
    "        \n",
    "        #print(urls[0:2])\n",
    "  \n",
    "    \n",
    "    #print job links of the current search result page:\n",
    "    print(\"total # of jobs on this page is :\"+str(len(urls)))\n",
    "\n",
    "    \n",
    "\n",
    "       \n",
    "    for url in urls: #change it back to for i in range(len(urls)): \n",
    "        get_info = True\n",
    "        try:\n",
    "            driver.get(original_url+url) #The driver.get method will navigate to a page given by the URL.\n",
    "        except TimeoutException:\n",
    "            get_info = False\n",
    "        except IndexError:\n",
    "            get_info = False\n",
    "            continue\n",
    "        j = random.randint(1500,3200)/1000.0 #change these values to avoid anti-scraping\n",
    "\n",
    "\n",
    "        time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "\n",
    "\n",
    "        if get_info:\n",
    "            soup=BeautifulSoup(driver.page_source)\n",
    "            \n",
    "            job_id_values=soup.find(\"div\",{\"class\":\"edit_note_content\"})\n",
    "            job_id=job_id_values['id'][11:]\n",
    "            #job_id_values data type is tag, to access a tag's attribute values use tag['attribute_key']\n",
    "            \n",
    "\n",
    "            print(\"job_id_values  is:\")\n",
    "            print(job_id_values)\n",
    "            print(job_id_values['id'])\n",
    "            print(\"job_id_values type is:\")\n",
    "            print(type(job_id_values))\n",
    "            print(\"job_id is:\")\n",
    "            print(job_id)\n",
    "            print(\"job_id type is:\")\n",
    "            print(type(job_id))\n",
    "\n",
    "           \n",
    "            job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "            print(job_title.text)\n",
    "\n",
    "\n",
    "            company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "            print(company.text)\n",
    "\n",
    "            location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "            print(location.text)\n",
    "\n",
    "            job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "\n",
    "            #print(job_description.text)\n",
    "\n",
    "            #print (driver.current_url)\n",
    "\n",
    "            jobs={\n",
    "                \"Job ID\":job_id,\n",
    "                \"Job Title\":job_title.text,\n",
    "                \"Company Name\":company.text,\n",
    "                \"Location\":location.text,\n",
    "                \"Url\":driver.current_url,\n",
    "                \"Job Description\":job_description.text\n",
    "\n",
    "            }\n",
    "            #This line inserts each job into MongoDB\n",
    "            print(jobs)\n",
    "            #collection.insert(jobs)\n",
    "            \n",
    "    \n",
    "#move to the next page of searched results, after iterating through all jobs links in current page\n",
    "    search_result_pg+=len(urls)\n",
    "    \n",
    "    print(page_number)\n",
    "    \n",
    "    print(searched_url) # print urls for search result page 1, 2 ,3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test a webpage\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "driver=webdriver.Firefox(executable_path=DRIVER_EXE)\n",
    "\n",
    "#Enter the url below\n",
    "\n",
    "test_url=\"https://www.indeed.com/rc/clk?jk=2188071b67e80bef&fccid=2b585dc0d549b736&vjs=3\"\n",
    "#test_url=\"https://www.indeed.com/viewjob?jk=fb2e3433eade4b6c&tk=1cf6na4o9btcr9g6&from=serp&alid=3&advn=8231918522092453\"\n",
    "    \n",
    "    \n",
    "#resp = requests.get(test_url)\n",
    "\n",
    "    #You can find out what encoding Requests is using, and change it, using the r.encoding property\n",
    "    #print(resp.encoding)\n",
    "    \n",
    "driver.get(test_url)\n",
    "\n",
    "soup=BeautifulSoup(driver.page_source)\n",
    "\n",
    "#print(soup)\n",
    "\n",
    "job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "print(job_title.text)\n",
    "\n",
    "\n",
    "company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "print(company.text)\n",
    "\n",
    "location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "print(location.text)\n",
    "\n",
    "job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "print(job_description.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fdb5c19c62d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcursor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mddddddd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collection' is not defined"
     ]
    }
   ],
   "source": [
    "cursor=collection.find()\n",
    "\n",
    "for doc in cursor:\n",
    "    print(doc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-ad78fe235aca>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-ad78fe235aca>\"\u001b[1;36m, line \u001b[1;32m62\u001b[0m\n\u001b[1;33m    ddddddd\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#by setting jobs=[] as a list at the beginning and then append dictionarys into it you create a list with jobs items.\n",
    "jobs=[]\n",
    "\n",
    "for i in range(len(urls)): #change it back to for i in range(len(urls)): \n",
    "    get_info = True\n",
    "    try:\n",
    "        driver.get(base_url+urls[i]) #The driver.get method will navigate to a page given by the URL.\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,2200)/1000.0\n",
    "    \n",
    "    \n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source)\n",
    "        print(soup)\n",
    "        \n",
    "        #head_tag=soup.head.title\n",
    "        \n",
    "        #title_tag = head_tag.contents[0]\n",
    "        \n",
    "        #print(title_tag)\n",
    "        \n",
    "        \n",
    "        #job_title=soup.b.string\n",
    "        \n",
    "        job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "        #print(job_title.text)\n",
    "        \n",
    "    \n",
    "        company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "        #print(company.text)\n",
    "        \n",
    "        location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "        #print(location.text)\n",
    "        \n",
    "        job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "       \n",
    "        #print(job_description.text)\n",
    "\n",
    "        #print (driver.current_url)\n",
    "        \n",
    "        jobs.append({\n",
    "            \n",
    "            \"Job Title\":job_title.text,\n",
    "            \"Company Name\":company.text,\n",
    "            \"Location\":location.text,\n",
    "            \"Url\":driver.current_url,\n",
    "            \"Job Description\":job_description.text\n",
    "            \n",
    "        })\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",'w') as f:\n",
    "    json.dump(jobs,f)\n",
    "        #collection.insert(jobs)\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",\"r\") as page:\n",
    "\n",
    "    #print(\"<>\".format(page.read()))\n",
    "    #page.seek(0) # for error \"JSONDecodeError: Expecting value: line 1 column 1 (char 0)\"\n",
    "    #data=json.load(page)\n",
    "    #print(data)\n",
    "\n",
    "    parsed=json.load(page)\n",
    "\n",
    "\n",
    "    collection.insert(parsed)\n",
    "\n",
    "\n",
    "\n",
    "pprint.pprint(collection.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15) \n",
    "\n",
    "#by setting jobs=[] as a list at the beginning and then append dictionarys into it you create a list with jobs items.\n",
    "jobs=[]\n",
    "\n",
    "for i in range(len(urls)): #change it back to for i in range(len(urls)): \n",
    "    get_info = True\n",
    "    try:\n",
    "        driver.get(base_url+urls[i]) #The driver.get method will navigate to a page given by the URL.\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,2200)/1000.0\n",
    "    \n",
    "    \n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source)\n",
    "        print(soup)\n",
    "        \n",
    "        #head_tag=soup.head.title\n",
    "        \n",
    "        #title_tag = head_tag.contents[0]\n",
    "        \n",
    "        #print(title_tag)\n",
    "        \n",
    "        \n",
    "        #job_title=soup.b.string\n",
    "        \n",
    "        job_title=soup.find_all(\"b\",{\"class\":\"jobtitle\"})[0]              \n",
    "        #print(job_title.text)\n",
    "        \n",
    "    \n",
    "        company=soup.find_all(\"span\",{\"class\":\"company\"})[0]\n",
    "        #print(company.text)\n",
    "        \n",
    "        location=soup.find_all(\"span\",{\"class\":\"location\"})[0]\n",
    "        #print(location.text)\n",
    "        \n",
    "        job_description=soup.find_all(\"span\",{\"id\":\"job_summary\"})[0]\n",
    "       \n",
    "        #print(job_description.text)\n",
    "\n",
    "        #print (driver.current_url)\n",
    "        \n",
    "        jobs.append({\n",
    "            \n",
    "            \"Job Title\":job_title.text,\n",
    "            \"Company Name\":company.text,\n",
    "            \"Location\":location.text,\n",
    "            \"Url\":driver.current_url,\n",
    "            \"Job Description\":job_description.text\n",
    "            \n",
    "        })\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",'w') as f:\n",
    "    json.dump(jobs,f)\n",
    "        #collection.insert(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The commented out lines \"page.seek(0) \"can fix the error: JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
    "with open(r\"C:\\Users\\Andrew Yan\\Documents\\GitHub\\HelloWorld\\jobs.json\",\"r\") as page:\n",
    "\n",
    "#print(\"<>\".format(page.read()))\n",
    "#page.seek(0) # for error \"JSONDecodeError: Expecting value: line 1 column 1 (char 0)\"\n",
    "#data=json.load(page)\n",
    "#print(data)\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
